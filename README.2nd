
This RQG version is focussed on taking care that
- single RQG test runs with
  - serious concurrency (threads > 1) and
    invoking KILL session, KILL QUERY, COMMIT/ROLLBACK RELEASE ....
    and a serious amount of DDL
  - intentional crashing of the server
  tend to a smaller fraction of false alarms and work more like wished
- problem analysis becomes more comfortable and faster
- parallel RQG test runs on the same box
  - have a (hopefully) zero risk to clash on the same ressources like ports
  - are hopefully unable to cause trouble via resource consumption
    except high CPU and IO load
- grammar simplification becomes faster
- some RQG run with "perl -w" throws significant less warnings than in history.

The "start" was a fork of https://github.com/elenst/rqg 2018-04-11.
The forking was done in order to avoid any thinkable damage to Elena's RQG
repository which is used in production.
Per my experiences with some other RQG version between 2012 - 2018 a lot
improvements to RQG were required and doable.

Please excuse
- any problems of my RQG when using it with
  - Windows
  - complicated usage variants of Combinations
  - optimizer tests
  - sophisticated setups with Galera, whatever replication, multi server setup etc.
  because I do not see me able to check any modification regarding usability in
  in these areas in the moment.
- that some
     git blame ....
  - shows me surprising frequent as last person who modified a line.
    I could not resist to adjust the code to my personal standards regarding readability.
    When looking for the last person making more probable significant changes use
       git blame -w .....
    which ignores at least changes around white spaces.
  - shows me sometimes as author of some file but it is 100% the work of Elena only.
    The reason for that is that I copied that file from one of her development trees
    into my tree.
- that it is extreme unlikely that
  - the commits to my RQG can be used for other RQG trees too
  - commits to other RQG trees can be used for my RQG trees
- that the branch
  - "master" is "frozen"
  - "experimental" is the one and only branch where development happens
    I use that branch including some not yet pushed changes for InnoDB QA.


Preparations for using RQG and my batch tool
--------------------------------------------
- Stuff needed for plain RQG and the mysqltest simplifier
  sudo apt-get install libdbi-perl
  sudo apt-get install libdbd-mariadb-perl
  sudo apt-get install libdbd-mysql
  sudo apt-get install libdbd-mysql-perl
  sudo apt-get install libtext-csv-perl

- Stuff needed for my batch tool rqg_batch.pl
  sudo apt-get install libfilesys-df-perl
  sudo apt-get install libsys-statistics-linux-perl
  sudo apt-get install liblog-log4perl-perl

  Prevent swapping because the resource control in rqg_batch.pl fears swapping goes towards
  on SSD and wants to avoid a wear out of that device by excessive writing.
  As root:
  - Solution which will disappear after the next reboot
    echo 0 > /proc/sys/vm/swappiness
    or
    swapoff -a

  - Solution which will survive the next reboot
    Edit /etc/sysctl.conf and add
    ###################################################################
    # For rqg_batch.pl
    # Make using the swap rare.
    #
    vm.swappiness = 0

    or remove the swap device which is my preferred solution.

  - Edit /etc/sysctl.conf and add for example
        fs.aio-max-nr=99999999
    and similar
        /etc/security/limits.conf
        * hard nofile 102400
        * soft nofile 51200
    in case the actual numbers for no of open files, aio, processes etc. are too low.


How to make a single RQG run
----------------------------
Please make a copy of the script RQG.sh and adjust it to your needs.
The log of the RQG run will be storage/SINGLE_RQG/rqg.log
The remains of the RQG run will be in /dev/shm/vardir/SINGLE_RQG .
Warning:
The next run with RQG.sh will drop and recreate storage/SINGLE_RQG and /dev/shm/vardir/SINGLE_RQG .


How to run a batch of RQG runs with massive parallelization
-----------------------------------------------------------
Please make a copy of
- the script BATCH.sh and adjust it to your needs
- the batch configuration file conf/mariadb/InnoDB_standard.cc and adjust it to your needs

The workdir of the last rqg_bat.pl run is
    last_batch_workdir == symlink pointing to storage/<highest timestamp>

Content of that workdir
    result.txt   Tabellaric overview about the RQG runs performed + the final verdict
    setup.txt    Tabellaric overview ... with full setup of the single RQG runs
    <number>.log rqg log of run <number>
    <number>.tgz compressed archive with the remains (data files, core,...) of run <number>
    some additional files

The verdict is made based on verdict_general.cfg.
Feel free to edit your version of it but never push that to the official repository.


How to get a verdict about the the result of some RQG run
---------------------------------------------------------
perl verdict.pl --log_file=last_batch_workdir/000001.log
which is equivalent to
perl verdict.pl --batch_config=verdict_general.cfg --log_file=last_batch_workdir/000001.log


Collection of hints (under construction)
----------------------------------------
1. Please be aware that the names of the exit statuses of the RQG runner (rqg.pl, runall*.pl ...)
   are not 100% accurate regarding some bad effect met or its reason.
   Sometimes the name is nothing more than a more or less qualified (~ pick the most likely
   reason) guess.
   Just one example of some thinkable scenario:
      (1) Prepare test ground                                   --> success
      (2) Start server                                          --> success
      (3) Whatever which finally causes that the server is down --> success
      (4) Start server (again)                                  --> fail
      ==> get STATUS_ENVIRONMENT_FAILURE
      (*) success == The expectations checked are fulfilled but some some deeper inspection
                     might show that already some states are not intended.
   (4) could for example fail because of
       a) The RQG runner made some internal error before.
          Than some STATUS_INTERNAL_ERROR would be more correct but the RQG runner
          is unable to throw that in case he has no consistency check indicating
          that he must have done something wrong.
       b) Something bad on the testing box happened like another MariaDB server
          occupies ports or required files were deleted or file system full etc.
          Than STATUS_ENVIRONMENT_FAILURE is quite perfect except somebody assumes to get
          this status only or already at begin of the test.
       c) In (3) the server misbehaved so that his data is non recoverable damaged.
          And that prevents to get success on server restart.
          Than STATUS_DATABASE_CORRUPTION or similar would be better.
   The reason why a STATUS_ENVIRONMENT_FAILURE might be thrown instead of some maybe better
   status might be:
   - A low level routine throws STATUS_ENVIRONMENT_FAILURE and the calling routines
     have no additional thoughts and just pass that status through.
   - The low level routine returns that it failed maybe including basic/low-level reason
     like file missing/crash/....
     Some caller routine transforms that status to what it assumes.
   I am working on making the status name reported more accurate.
2. The status STATUS_SERVER_CRASHED and/or its use is frequent very misleading up till plain wrong.
   There are many different reasons why the communication between the client and some server
   does not work like expected like timeout for response exceeded because
   - the server has really crashed
   - the server was crashed intentional
   - the server has not crashed but somehow hangs/is no more responsive
   - the server is healthy but under too high load for the timeouts given
   - shortages in free OS ressources etc.
   and so on.
   IMHO routines need to have some status for describing communication trouble with unclear reason.
   But the name for that should be rather STATUS_COMMUNICATION_TROUBLE.
   STATUS_SERVER_CRASHED should be only used if
   - we have a core file or
   - the process of the server within the OS has disappeared.
   But even than a clean distinction between
   - real server crash because of server code failure -- ok, STATUS_SERVER_CRASHED
   - real server crash because of intentional server kill -- ok, STATUS_SERVER_KILLED
   - real server crash because of misbehaviour of concurrent programs on testing box
   - real server crash because of OS kills server because of whatever reason
   - intentional and simple shut down of the server
   is quite complicated.
3. My preference regarding formatting in order to improve readability:
   - removal of trailing white spaces
   - line length <= 100 characters
     Exception: Comments containing fragments of rqg or server error log
   - no use of tabs because authors frequent
     - mix tabs and spaces which looks than ugly in case you go with some different tabstop value
     - did not agree on how many spaces a tabstop should be
   - indentation unit of 4 white spaces
     I am aware that a few spaces more would increase the readability.
     But more spaces would also frequent cause that the line length limit of 100 is reached and
     than we need serious more lines for the logical same code.
   - Avoid dense code like: $m+1 or $part1.$part2."ABC"
   - Comments must have one or more white spaces after '#'
   - Placing '=', '.', ';', '=>' for several text lines in the same column helps frequent to
     avoid forgotten chars, accidents like ':' instead of ';' and similar.
4. Other preferences
   - Messages should be mostly complete sentences.
   - Messages about serious failures should start with "ERROR: "
   - Messages about important states reached should start with "INFO: "
   - Especially in case of serious bad statuses handed from low level (1) routines to high level
     routines (2) we could have transformations of the status. Such intentional transformations
     need to get reported.
     We need to understand why some lost connection, DBSTATUS_ERROR etc. finally leads to
     that the RQG runner exits with STATUS_ENVIRONMENT_FAILURE ro similar.
     (1) Low level routines are usually quite good in "describing" detailed what happened
         physically. But they also usually lack the overview for judging if that is an error
         (deviation from expectation) or not.
     (2) High level routines call low level routines and get usually statuses returned.
         These statuses lack often details which were known to the low level routine.
         So we have some loss of information.
         But high level routines have also some better overview about what is to be expected
         than low level routines. So they can better judge if they have met an error
         including giving some qualified guess about the most lileky reason.
   - Try to use some more accurate wording.
     Call a vague though qualified guess a "guess".
     Feel free to
     - give a list of guesses per event of interest
     - call a guess which is with a likelihood of lets say >= 95% true "truth"
   - A bunch of extra output lines, lets assume 100 per RQG run, which start with "INFO: "
     and allow to conclude in which phase of the RQG test work flow we are might help a lot
     in failure analysis.
   - As soon as its obvious that GenTest will have to exit with a status code > 0 the amount
     of extra information is allowed to be big.
   - Make messages "grep friendly"
   - Reduce the use of "croak"
     - We might want some more specific exit status than just "2".
     - Carp::cluck gives frequent a serious better information how we reached the routine
       where the problem showed up.
     - There might be cases where the immediate exit prevents to make some cleanup like
       throw valueless files away or stop running servers or similar.
       Whatever test runs being executed in parallel or successing might be serious harmed
       by such a missing cleanup.
       Typical bad effects of more or less serious extend:
       - Server start fails because there is already some old server using that port.
       - Short or long timespans with masses of zombie processes
       - A client which belongs to some old long gone test fiddles in the server of our
         current test.
       - Reach tmpfs full far way more often
       - Server for test Y starts somehow on data belonging to the the server of the
         already gone test X instead of his own data.
5. Easy code failure
   my $config = GenTest::Properties->new(
       options =>  $options,
       legal   =>
           [
                'config',
                'input_file',
                ....
           ],
       required =>
           [
                   'basedir',
                   ...
           ],
       # Attention {} and never [] because that leads to
       # Not a HASH reference at lib/GenTest/Properties.pm line ~ 125
       defaults =>
           {
                   'parallel'          =>  1,
                   ...
           },
   );
6. Never run something like
   my ($ret, $args) = Getopt::Long::GetOptionsFromString($cl_snip,
       'runner=s'                 => \$runner_passed_back,
   );
   because it will
       wanted     remove some  '--runner=<value>'
       disaster   remove single quotes from values within $cl_snip.


DONE (incomplete and arbitrary sorted list):
--------------------------------------------
1. Introduce the capability to define and process variables keeping lists with
   - RQG exit statuses which are desired/replay or of interest or unwanted
   - text patterns which are desired/replay or of interest or unwanted
   Implement a program which processes config file and log of a RQG run and gives a verdict based
   on these lists. --> verdict.pl , lib/Verdict.pm , verdict_general.cfg
2. Implement some new RQG runner (rqg.pl) which
   1. performs a RQG run according to the setup supplied via call/command line
   2. calculates the verdict based on balck/whitelist matching after the RQG run finished
   3. performs a (hopefully) perfect cleanup including archiving according to the verdict got
4. Implement a new RQG tool (rqg_batch.pl) which performs a batch of RQG runs according to the
   setup supplied via config file and command line.
   Massive parallelization is supported too.
   This tool replaces (at least for my tasks)
      bughunt.pl
      combinations.pl
      util/simplify-grammar.pl
   Certain features for
   - efficient job management are located in lib/Batch.pm
   - resource control for preventing disasters on the testing box are in lib/ResourceControl.pm
   rqg_batch.pl uses some configurable RQG runner. The default is rqg.pl.
5. Implement some new grammar simplifier (lib/Simplifier.pm etc.) which exploits the capabililties
   provided by rqg_batch.pl and rqg.pl.
6. rqg.pl, rqg_batch.pl follow some extreme strict regulations regarding placement of files,
   cleanup etc.
7. Extend the grammar simplification algorithm so that all top level rules get simplified.
   Old state: Only the rule 'query' gets considered up till that the simplifier might
              not work if other top level rules exist and play an important role.
   New state: All currently supported top level rules get considered.
              query, thread<n>, *_init, *_connect
8. Introduce the grammar rule class *_connect. To be executed for every (re)connect.
   This was required for better testing of SQL affected by KILL QUERY, KILL <session> issued
   by some concurrent session etc.
9. Certain improvements up till fixes within the RQG core in order to support RQG concurrency
   testing, testing with massive parallelization better and to reduce the fraction of false alarms.
10. Implement routines which cause that the final gendata, gentest call works
    - with the files
        rqg.yy , rqg.sql (might not exist), rqg.zz (might not exist)
      only
    - without giving $no_mask, $mask, $mask_level any attention because any impact of masking
      is already in the content of rqg.yy
    This is required for the grammar simplifier but gives also advantages in other situations
    like archiving of data for replay attempts in future.
11. Correct the simplifier algorithm (--> lib/GenTest/Simplifier/Grammar_advanced.pm)
    The sometimes illegal actions:
    1. The simplifier tries to remove even the last non "empty string" component/alternative
       of a grammar rule.
       Examples:
          rule_1 : SELECT 13 ; # last non "empty string" component which would get "attacked"
                               # == attempt to remove the value!

          rule_2  : ;          # last "empty string" component. The simplifier would inline
                               # that rule and by that the empty string == no removal of value.

    2. The simplifier tries to remove parts of components.
       Example observed:
          Snip of some component: , REPEAT ( CAST ( $my_int AS CHAR(1)), @fill_amount ) )
          Attempts to remove (between the arrows):
          ->) ) ;<-  ->fill_amount<- ->)), @<- ->1<- ->(<- ->CHAR<- -> <- ->AS<- -> <-
          ->$my_int<- ->( <- ->CAST<- ->(<- ->REPEAT<- ->,<-
    Both cases could be tolerated in case we simplify a crash but they are plain illegal in case
    we simplify for something different.
    Example of evil impact in some artificial scenario:
        rule_no_change:
            rule_1(modify data) ; rule_2(revert the modification);
        followed by validate that the sequence has "Something" not changed.
        "Something": Number of rows in table A or SUM(column_a) or
                     column_b WHERE column_pk = 13 or ...
        And we had the interesting case that there was a change though it should not.
        The old simplifier would maybe shrink one of the rules 'rule_1' or 'rule_2' to "empty"
        and than the validator will kick in and "cry" his alarm which gets than valuated as success
        (replay) in simplification.
        Impact: The final result of grammar simplification will be nothing else than garbage.
    This happens frequent but there must be some simplification algorithm which could be used
    for all the other cases too.
    Now we have some non destructive (default) and some destructive grammar simplification mode.
12. Use within the grammar simplifier some more dynamic amount of walk through the grammar rounds
    (--> lib/GenTest/Simplifier/Grammar_advanced.pm)
13. Develop a program which is capable to check if the protocol of some RQG run matches the
    wanted/unwanted statuses/patterns setting provided.
    (--> verdict.pl)
14. https://jira.mariadb.org/browse/MDEV-16863 Extend the RQG infrastructure for backup testing
    Reporter: Mariabackup
    1. Hot backup of data of running server 1
    2. Prepare the data backed up for restore + restore to other place than server 1
    3. Start some additional server 2 on that data
    4. Run check table in server 2
    5. If no trouble during 4. shutdown of server 2 + destroy his data + run next loop round.
       If trouble terminate the test.
15. Job queues (--> lib/Batch.pm)
16. Simplification
    Having n RQG tests with different grammars
        They are all slightly shrinked derivates of in history successful grammars.
    in parallel could culminate in more than one grammar replaying the desired outcome.
    We can only pick one of them.
    In case we memorize (requires bookkeeping) what the second replayer Y made like
         That run tried with
         dml:
             INSERT ... |
             UPDATE ... | <======= This removed
             DELETE ... ;
    than we could apply that in some next grammar simplification attempt again including
    to give such runs a higher priority.
17. Modify the grammar simplifier algorithm so that we need the same number of grammar
    simplification steps no matter if GRAMMAR_FLAG_COMPACT_RULES is applied or not.
      Example:
      rule1: update | insert | delete | insert ;

      Old algorithm (GRAMMAR_FLAG_COMPACT_RULES is default)
      -----------------------------------------------------
      With GRAMMAR_FLAG_COMPACT_RULES use as base:    rule1: update | delete | insert ;
      and try in worst case 3 steps
          rule1: update | delete          ;
          rule1: update |          insert ;
          rule1:          delete | insert ;
      Without GRAMMAR_FLAG_COMPACT_RULES use as base:    rule1: update | insert | delete | insert ;
      and try in worst case 4 steps
          rule1: update | insert | delete          ;
          rule1: update | insert            insert ;
          rule1: update |          delete | insert ;
          rule1:          insert | delete | insert ;
      Note: The number of steps depends a lot on the shape of the grammar.

      New algorithm (default is "no" transformation of grammar)
      ---------------------------------------------------------
      Use as base:    rule1: update | insert | delete | insert ;
      and try in worst case
          rule1: update |          delete          ;
          rule1: update | insert |          insert ;
          rule1:          insert | delete | insert ;
      So the historic advantage of GRAMMAR_FLAG_COMPACT_RULES when using the old simplifier
      algorithm does no more exist with the new one.

      Advantages/disadvantages of the new algorithm + default per already existing experience:
      a) Simplification speed
         Depending on the test GRAMMAR_FLAG_COMPACT_RULES could be an advantage but also a
         disadvantage.
         In average there is a minor advantage when not applying GRAMMAR_FLAG_COMPACT_RULES.
      b) Evolution of resource consumption during grammar simplification
         There could be some drastic change of resource consumption (especially the space required
         in vardir) to the bad but also the good.
         Given the facts that
         - (unlikely) the rqg_batch.pl parameter "--parallel" might be tweaked to avoid some
           dangerous resource consumption and the corresponding experience was taken from
           previous tests with the original grammar
         - (all time) the rqg_batch.pl ResourceControl works better when having
           - a bigger amount of experiences (tests already run since start of rqg_batch.pl)
             At the point of progress where GRAMMAR_FLAG_COMPACT_RULES is applied if
             assigned at all both variants (with/without GRAMMAR_FLAG_COMPACT_RULES) have the
             same rather limited amount of experiences.
           - slowly changing grammars
             And here makes the application of GRAMMAR_FLAG_COMPACT_RULES some unusal big change.

      Using GRAMMAR_FLAG_COMPACT_RULES is no more the default and no more recommended.
      But there are also up till today no experiences which show that GRAMMAR_FLAG_COMPACT_RULES can
      be serious bad.
      Feel free to experiment.
18. Remove empty statements from grammars.
    Example:   SELECT 13 ; ;
19. Add some simple error tagging
    Example:
    wanted pattern
    [ 'MDEV-20775' , 'InnoDB: Failing assertion: \!page_zip \|\| page_zip_validate\(page_zip, page, index\)' ],
    In case you hit a server crash + the RQG log contains the text pattern than the file
    last_batch_workdir/result.txt will contain an entry like
    2020-01-17T18:13:14 | Number | Worker | Verdict          | RQG log    | OrderId | RunTime | Derivate used      | Parent of derivate | Extra_info
    ...
    2020-01-17T18:25:00 |    284 |     67 | replay           | 000283.log |     178 |      72 | c00193.yy          | p00000.yy          | STATUS_SERVER_CRASHED--MDEV-20775
20. Simplify also the amount of reporters, validators, transformers and threads.
    Open:
    Whatever simplification (grammar, reporters, ...) goes with steps doing different things.
    - So which order of steps to use?
    - How to avoid that we trade simplification and replay speed for simplicity?


TODO (incomplete list):
-----------------------
   1. Add maybe the option to not remove components of rules which contain the keywords DROP,
      TRUNCATE or DELETE.
      Background:
      - As soon as we remove such components we are non rare faced with some serious growth of the
        storage space consumption which is counter productive in many aspects
        1. It might cause some OS breakdown in case of placing the vardir on a filesystem of type
           tmpfs as long as we have no load/resource control mechanism which prevents trouble.
        2. Especially without but also with a load/resource control mechanism we are faced with
           that the current MariaDB and also MySQL the server tend to assert with
              InnoDB error 28 (no space on disk)
           and the debugger analyzing the core file (reporter Backtrace) reports frequent
              BFD: Warning: .....data/core is truncated:
                            expected core file size >= 959647744, found: 55398400.
           I have no fixed opinion regarding if its better to assert with core or to make something
           like an emergency shutdown of the server without core.
           Even a clever RQG resource control and emergency shutdowns cannot prevent that maybe
           even one RQG test with one server consumes all available storage space.
           And the result of the test is of no value.
      - The simplifier produces grammars which are capable to replay but start to need to require
        far way more test runs.
        Example:
        Assume that some assert happens during CREATE TABLE <not yet existing table> meeting certain
        concurrent SQL. The simplifier will detect that some corresponding DROP TABLE <sample table>
        is not required for replaying the assert. Hence it will shrink the DROP away.
        But that leads to the following change in properties to replay
        1. The average runtime of replaying runs decreases negligible.
        2. We need serious more replay attempts because we either replay with some extreme short
           RQG run or never in this run no matter how long it lasts because after some one and only
           passing CREATE TABLE any future CREATE TABLE will fail because that table already exists.
      I think already since some time about
      - first phase A of simplification:
        "Do not attack SQL containing DROP (schema, table, key, trigger, .... ) or DELETE at all.
        Final result of this phase is grammar a.
        Warning regarding how to do that:
        Filtering out simplifications like "remove from rule X the component DROP ..." is not
        sufficient because we could also have "remove from X the component c" and c contains
        the rule Y which contains the DROP.
        Idea: Compare parent P<m> grammar to potentional tried grammar T<n> regarding the amount
              of DROP and DELETE. In case there are less in T<n> than T<n> removed some.
        Problem: Case insensitive drop as Rule name or just word of statement.
                 But I assume extreme perfection in this case is not required.
      - later phase B:
        Take grammar a and attack DROP/DELETE but check for any shrinked grammar which replays if
        the likelihood to replay has dropped. If yes, than revert this simplification.
        Final result of this phase is grammar b.
      - later phase C or somehow joined with B:
        Take grammar b and now attack DROP/DELETE too.
        Final result of this phase is grammar c.
      How to use these grammars later:
      - ignore or drop grammar a. Depending on point of view either grammar b or c is better.
      - Present the bug fixer grammar c as example which SQL the bug needs
      - From now on use grammar c for any replay attempt. Its the most efficient.
      Some prototype code exists but I am not convinced of it.
   5. The first step within the grammar simplification process is:
      Try to replay with the original YY grammar and give up in case that goal was not achieved.
      Handle this amount of trials better if required.
-- Improve RQG components like reporters and validators as soon as I am forced to use them and
   meet trouble.
   This might sound banal but per my experience many parts of RQG are the opposite of well prepared
   for meeting the rough conditions of tests invoking KILL QUERY up till KILL <session> and
   that even on some heavy loaded testing box.
-- Ugly observation (2018-08)
   During grammar simplification range_access.yy shows some disastrous runtime behaviour.
   After quite short runtime the perl processes for the threads consumed more memory than
   the DB server. I guess this problem is caused by the "stack" feature of RQG.
   No idea who will fix that problem.
-- Describe the work flow from
   1. bug hunting with RQG tests (rqg_batch.pl with cc config file)
   2. grammar simplification for one or more of the bugs found in 1.
   3. How to develop ibased on 2. some final replay testcase which is maybe MTR based.
-- Find some way to preserve the server binaries used.
   Purpose:
   Lets assume at some point of time after the test run the binary gets modified or the
   complete build gets thrown away.
   But the analysis of core files requires corresponding binaries + reconstructing them is
   some costly process and quite error prone (What if local non committed modifications?).
   Hence preserving at least the server binaries used is recommended.
   Letting rqg.pl do that is thinkable.
   The difficulties show up as soon as
   - more than one binary is used per test (upgrade or replication)
     How to name/mark different binaries?
   - batches of tests are performed
     How to
     a) avoid to preserve duplicates of binaries (storage space + efforts (time, CPU, IO) at runtime.
     b) avoid the overhead of
        We run a batch of RQG tests with the same basedirs.
        All rqg.pl preserve the binary in their work directory. (time, CPU, IO at runtime)
        rqg_batch.pl acts according to a) and throws duplicates away but needs to somehow
        preserve the knowledge the test X was using preserved binary x.
     c) somehow minimize that the rqg_batch.pl needs to act
        - in advance and analyze what some RQG run is going to do (basedir etc.)
        - after some RQG run and analyze results
        because that
        - must not cause that rqg_batch.pl loses its control over the active RQG runner
          Just assume rqg_batch.pl exits when hitting a problem.
        - should not cause to lose its control over too long timespans.
          Assume some long lasting analysis or copy process.
          During that controlling the RQG runner is with the current concept impossible.
     rqg.pl
     - creating files outside of his workdir and vardir
     - inspecting files (example: server binaries already stored by rqg_batch.pl)
       outside of $RQG_HOME
     is strict out of discussion.
     Idea:
     rqg.pl could store the binary identifying information (location + md5sum, maybe GIT info) in
     some file within its workdir.
     rqg_batch.pl could process that file and preserve binaries and whatever information if
     not already done for some finished RQG run.
     IMHO acceptable weak point: Modification of binary during rqg_batch.pl runtime.
-- Let rqg_batch.pl run mysqltest simplification
-- Implement several optional "strangler" which cause that some ongoing simplifications attempt
   is stopped in case it is extreme likely that we will get no more a replay with this one.
   Criterions:
   - up till now elapsed RQG/MTR runtime (one tool call running a campaign of tests)
   - up till now elapsed runtime for a RQG/MTR test executed by a Worker within a tool
   - actual size of output
   Maintain a fifo with n elements whenever a replay was achieved.
   Compute a value giving a > 85% percentil and use that as upper limit for the next to be
   started RQG/MTR run. Stop that run as soon as that limit was exceeded and maybe charge into
   the fifo a value like limit * 1.01.
-- Ideas for some improved mysqltest simplifier
   Some first note:
   The algorithm (--> Andreas Zeller) of the current mysqltest simplifier is extreme efficient
   but lacks support for parallelization.
   I am aware that 2. is in some points maybe less efficient but supports parallelization.
   1. Try with parallel=1 and repeat=1.
      In case that fails than there might be two reasons
      a) the search pattern is wrong
      b) unknown but per more than one time observation going with
         - repeat > 1 helped sometimes but often not
         - In all cases where repeat > 1 helped a doubling of the test code helped too.
         My guess: It was a problem in InnoDB purge.
         The original test was derived from some rqg_batch run and that probably
         - generates per RQG worker -> server more log than per MTR worker -> server
         - runs more SQL -> need more time than in MTR
         - runs with more parallel CPU load on testing box  SQL -> need more time than in MTR
         So whatever criterion purge uses for deciding to become active, there is sufficient
         in RQG but not in the derived MTR Test.
      So if no replay
      -  Try with doubled script code.
      -  Try with quadrupled script code.
      If even this does not replay than give up.
   2. When having some "base" test
      Set $parallel to user defined value or the default `nproc`.
      Set $dividor = $parallel.
      $replay_in_round = 1;
      Loop (if 1 == $replay_in_round)
          $replay_in_round = 0;
          Add x empty lines to the last replaying script so that  no of lines MOD dividor = 0.
          Make dividor derivates
              First derivat = Last replayer with the first 1/dividor lines removed.
              Last  derivat = Last replayer with the last  1/dividor lines removed at end.
              Naming of tests: SD_<iteration with left side zeros>-<1 till dividor>
          Loop as long as (combine with "and")
              - no replay achieved
              - $dividor < 2 * number of line in replaying test
                Goal: The ~ last round should be : Attack any single code line in test.
              ./mtr --parallel=$parallel ... --do-test=SD_<iteration with left zeros>
              If replay
              - YES: Catch the name of the test and declare it to be the last replaying script.
                Delete all other non replaying scripts SD_<iteration with left zeros>_...
                $replay_in_round = 1;
                (perl) last    (*)
              - NO: $dividor = 2 * $dividor
                (perl) last
      -----------------------------------------------------------------------------
      Problems(test it out):
          Is it possible to identify the replaying test with 100% accuracy?
          V1.test == V2.test with content
          --disable_abort_on_error
          CREATE TABLE t1 (col1 INT);
          A result file does not exist.
          We get a pass no matter how many repeats I run.
   3. Try competing
      -filters
       - failing statements removed
       - SHOW
       - SELECT
       - ???
      - test setups which make the run faster
        - no warnings
        - fast shutdown
        - ......
- new simplification_phase
  grammar_disarm ?
  Have a configurable list of keywords like XA, FOREIGN KEY, references, ... ordered
  by how much it would be appreciated that this feature is not required for replaying
  some badd effect.
  Make order ids along that list.
  Have in the derivate grammar that keyword mangled or the rule component mangeled or
  the rule component set to '' because the generated statement will be most probably
  rotten anyway.

--dryrun=... works quite incomplete because best_grammar.yy is not fully maintained.


The architecture around rqg_batch.pl
------------------------------------
rg_batch.pl
0. processes certain options from his command line
1. initializes the Combinator or Simplifier by passing a sub set of the options through
   Some main mandatory option is the rqg_batch.pl config file which gets processed by
   the Combinator or the Simplifier.
2. as soon as there are free resources rqg_batch "asks" the Combinator or Simplifier for some
   job and gets than some fragment of a RQG command line (grammar, threads etc. assignment)
   returned.
3. complements the fragment with parameters which guarantee that a to be started RQG runner
   does not clash with his resources (vardir, workdir, ports, files) with other RQG runners
   in parallel.
4. forks some corresponding RQG Worker
   This Worker does some preparations and makes the RQG run, getting the verdict etc. via "system".
5. "reports" the outcome of any finished job to the Combinator or Simplifier so that they
   can update their bookkeeping and adjust their responses (see 2.) according to the
   progress currently achieved.
6. observes the state of the system and prevents certain disasters
      Example: The vardir for the RQG batch run is frequent located on some RAM based
               filesystem which has usually some quite limited size.
               So a filesystem full is some real risk.
   and stops RQG runner in order to prevent them.
   The "stopped" run will get repeated if required and as soon as possible.
   - The resource control is implemented.
   - The repetition of stopped RQG runs is implemented.


Hints
---------------------------------------
1. Typical errors in Simplifier config file
1.1 String found where operator expected at (eval 25) line 102, near "'STATUS_CONTENT_MISMATCH'
           'STATUS_CRITICAL_FAILURE'"
        (Missing operator before
           'STATUS_CRITICAL_FAILURE'?)
    Unable to load storage/<timestamp>/simplifier.cfg:
        syntax error at (eval 25) line 102, near "'STATUS_CONTENT_MISMATCH'

    Reason:
    whitelist_statuses => [
            'STATUS_REPLICATION_FAILURE',
            'STATUS_CONTENT_MISMATCH'      <== The ',' afther the 'STATUS_CONTENT_MISMATCH' is
            'STATUS_CRITICAL_FAILURE'          missing and there follows '<something>'.
    ],

    Recommendation:
    Append all time a ',' to members of lists.

2. Typical errors in RQG YY grammars
2.1 Rules must have a lower case name.
    Otherwise they will not get valuated as the name of a rule.
2.2 Separating the components of a rule is intented but instead of the correct '|' a ';' is used.
    Negative example:
    rule1:
        t3 ;
        t4 ;
    query:
        SELECT col1 FROM rule1;
    leads to the generation of queries like
    a)  SELECT col1 FROM t3 ; <-- correct syntax
    b)  t4 ;                  <-- wrong syntax
3. Take care that the content of literals is not equal to a rule name.
   Negative example:
    rule1: { $var = 'rule1' ; return undef };

How to use "rr" (https://github.com/mozilla/rr) in combination with RQG
WARNING: THE DESCRIPTION IS SLIGHTLY OUTDATED
-----------------------------------------------------------------------
0. The current invocation of "rr" into RQG switches the generation of core files off.
   All info required is in the "rr" traces + the core files occupy too much storage space.
1. The OS needs to be a Linux.
   Ubuntu:     sudo apt-get install rr
   Depending on the version of your distribution you get some more or less actual "rr".
2. Depending on actual setup/state of the testing box
   sudo "echo 1 > /proc/sys/kernel/perf_event_paranoid" # --> temporary setting
   or
   Append to /etc/sysctl.conf                           # --> permanent setting
        kernel.perf_event_paranoid = 1

   # Required on some server with Ubuntu 17.04 but not on my notebook with Ubuntu 17.10.
   sudo apt-get install cpufrequtils
   sudo cpufreq-set -g performance                      # --> temporary setting
3. In case you know of a way that the server writes more messages into the server error log
   than take care that its enabled. The goal is to get many entries with rr event number.
   Unfortunately "--log-output=file" does not cause the nice to have positive impact.
4. rqg_batch.pl call
   - my notebook with Ubuntu 17.10 and rr version > 4
     perl ./rqg_batch.pl ... --rr  <no assignment of rr_options> ....
   - some server with Ubuntu 17.04 and rr version 4
     The plain rr cannot figure out which type the CPU is.
     - cat /proc/cpuinfo shows ...
       vendor_id       : GenuineIntel
       cpu family      : 6
       model           : 85
     - Page in web mentions that its a 'Intel Skylake'.
     - strings -a /usr/bin/rr | grep -i Sky     shows a match
     perl ./rqg_batch.pl ... --rr --rr_options="--microarch='Intel Skylake'"
     worked well
   Note:
   - I have no experiences how good all this works on AMD or ARM CPU's.
   - In case "rr" meets trouble than the server error log might contain an error message like
     - # 2020-01-13T19:11:48 [45598] | [rr 46342 9202]2020-01-13 19:11:47 0 [Note] mysqld: O_TMPFILE is not supported on /dev/shm/vardir/1578939104/2/1/tmp (disabling future attempts)
       # 2020-01-13T19:11:48 [45598] | [FATAL /build/rr-2h5IId/rr-4.4.0/src/record_syscall.cc:4180:rec_process_syscall_arch() errno: SUCCESS]
       # 2020-01-13T19:11:48 [45598] |  (task 46342 (rec:46342) at time 9224)
       # 2020-01-13T19:11:48 [45598] |  -> Assertion `t->regs().syscall_result_signed() == -syscall_state.expect_errno' failed to hold. Expected ENOSYS for 'io_setup' but got result 0 (errno SUCCESS); execution of syscall unsupported by rr
       # 2020-01-13T19:11:48 [45598] | [FATAL /build/rr-2h5IId/rr-4.4.0/src/log.cc:283:emergency_debug() errno: SUCCESS] (session doesn't look interactive, aborting emergency debugging)
       # 2020-01-13T19:11:48 [45598] | Aborted (core dumped)
       loose_innodb_use_native_aio=0                 helped.
     - # [FATAL /build/rr-2h5IId/rr-4.4.0/src/PerfCounters.cc:138:get_cpu_microarch() errno: ENOTTY] CPU 0x50650 unknown.
       --rr_options="--microarch='Intel Skylake'"    helped.

5. Get the archive and unpack what is needed
   mleich@skylake01:/work/RQG_mleich_N$ grep replay last_batch_workdir/result.txt
   2020-01-20T17:54:10 ---------- first_replay ---------- (c00000.yy)
   2020-01-20T18:03:24 |    114 |     95 | replay           | 000113.log |      95 |     354 | <undef>            | <undef>            | STATUS_SERVER_CRASHED--MDEV-20775
   STATISTICS:        1 -- 'replay'           -- Replay of desired effect (Whitelist match, no Blacklist match)
   RESULT:     The best verdict reached was : 'replay'

   "rr" memorizes where which file (mariadb source+binaries) was located.
   There is some sophisticated functionality which allows to export stuff to other
   boxes for debugging. But RQG does not use that in the moment.
   Also the tracing files <whatever1>/rr_trace/<whatever2> seems to memorize other required
   tracing files via absolute paths.
   So the content of the archive of the RQG run needs to get unpacked to its original place.

   cd /
   mleich@skylake01:/$ ls -ld /work/RQG_mleich_N/last_batch_workdir/000113.tgz
   -rw-rw-r-- 1 mleich dev 119948549 Jan 20 18:03 /work/RQG_mleich_N/last_batch_workdir/000113.tgz # Just 12 MB !!

   mleich@skylake01:/$ tar xvzf /work/RQG_mleich_N/last_batch_workdir/000113.tgz dev/shm/vardir
   dev/shm/vardir/1579539250/95/
   dev/shm/vardir/1579539250/95/1/
   dev/shm/vardir/1579539250/95/1/mysql.log
   dev/shm/vardir/1579539250/95/1/mysql.err
   dev/shm/vardir/1579539250/95/1/rr_trace/
   dev/shm/vardir/1579539250/95/1/rr_trace/latest-trace
   dev/shm/vardir/1579539250/95/1/rr_trace/mysqld-0/
   dev/shm/vardir/1579539250/95/1/rr_trace/mysqld-0/version
   ....
6. Debugging
   export _RR_TRACE_DIR=dev/shm/vardir/1579539250/95/1/rr_trace

   mleich@skylake01:/$ rr ps
   PID     PPID    EXIT    CMD
   325332  --      -6      /work/10.2/bld_debug/sql/mysqld <a lot stuff> --sql-mode=no_engine_substitution
   334471  325332  0       addr2line -C -f -e /work/10.2/bld_debug/sql/mysqld
   334517  325332  0       addr2line -C -f -e /lib/x86_64-linux-gnu/libpthread.so.0
   334522  325332  0       addr2line -C -f -e /lib/x86_64-linux-gnu/libc.so.6
   334555  325332  0       addr2line -C -f -e /work/10.2/bld_debug/sql/mysqld
   334617  325332  0       addr2line -C -f -e /lib/x86_64-linux-gnu/libpthread.so.0
   334622  325332  -9      addr2line -C -f -e /lib/x86_64-linux-gnu/libc.so.6
   # Great it somehow works.
   # Feel free to pick a pid.

   mleich@skylake01:/$ rr replay --microarch='Intel Skylake' # The server used for testing
   or
   mleich@skylake01:/$ rr replay                             # My notebook
   or in case you do not want to flip between pids during debugging
   mleich@skylake01:/$ rr replay -p <pid you are interested in>

   GNU gdb (Ubuntu 7.12.50.20170314-0ubuntu1.1) 7.12.50.20170314-git
   Copyright (C) 2017 Free Software Foundation, Inc.
   ...
   Remote debugging using :50956
   warning: unable to open /proc file '/proc/325332/task/325332/maps'
   warning: remote target does not support file transfer, attempting to access files from local filesystem.
   Reading symbols from /lib64/ld-linux-x86-64.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/ld-2.24.so...done.
   done.
   0x00007fda8c9e6c20 in _start () from /lib64/ld-linux-x86-64.so.2
   (rr) when
   Current event: 15     # We are currently on rr's event number 15
   (rr) continue
   Continuing.
   2020-01-20 17:57:32 140576640939840 [Note] /work/10.2/bld_debug/sql/mysqld (mysqld 10.2.31-MariaDB-debug-log) starting as process 325332 ...
   ....
   2020-01-20 17:57:33 140576640939840 [Note] /work/10.2/bld_debug/sql/mysqld: ready for connections.
   Version: '10.2.31-MariaDB-debug-log'  socket: '/dev/shm/vardir/1579539250/95/1/mysql.sock'  port: 28480  Source distribution
   [New Thread 325332.325545]
   2020-01-20 17:57:47 140575085319936 [Note] InnoDB: Sync to disk of `test`.`oltp2` started.
   2020-01-20 17:57:47 140575085319936 [Note] InnoDB: Stopping purge
   2020-01-20 17:57:50 140575085319936 [Note] InnoDB: Writing table metadata to './test/oltp2.cfg'
   2020-01-20 17:57:50 140575085319936 [Note] InnoDB: Table `test`.`oltp2` flushed to disk
   2020-01-20 17:57:52 140575085319936 [Note] InnoDB: Deleting the meta-data file './test/oltp2.cfg'
   2020-01-20 17:57:52 140575085319936 [Note] InnoDB: Resuming purge
   2020-01-20 17:58:38 0x7fda480ac700  InnoDB: page_zip_validate: page header
   page_zip:
   0000 0090a975da7f00006e00000091000160
   ....
   1fe0 0000000000000000000000000000000000000000007000630000000000000000
   2020-01-20 17:58:39 0x7fda480ac700  InnoDB: Assertion failure in file /work/10.2/storage/innobase/btr/btr0cur.cc line 1429
   InnoDB: Failing assertion: !page_zip || page_zip_validate(page_zip, page, index)
   ....
   [New Thread 325332.326638]

   Thread 2 received signal SIGABRT, Aborted.
   [Switching to Thread 325332.325545]
   __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:58
   58      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory
   (rr) when
   Current event: 295768    # <-- Highest event number of interest
   (rr)
   ###### Just for comparison the /dev/shm/vardir/1579539250/95/1/mysql.err shows
       [rr 325332 6262]2020-01-20 17:57:32 140576640939840 [Note] /work/10.2/bld_debug/sql/mysqld (mysqld 10.2.31-MariaDB-debug-log) starting as process 325332 ...
       ....
       [rr 325332 14527]2020-01-20 17:57:33 140576640939840 [Note] /work/10.2/bld_debug/sql/mysqld: ready for connections.
       ....
       [rr 325332 167600]2020-01-20 17:57:52 140575085319936 [Note] InnoDB: Resuming purge
       [rr 325332 269032]2020-01-20 17:58:38 0x7fda480ac700[rr 325332 269035]  InnoDB: [rr 325332 269037]page_zip_validate: page header
       [rr 325332 269039]page_zip:
       ....
       [rr 325332 295745]2020-01-20 17:58:39 0x7fda480ac700[rr 325332 295748]  InnoDB: Assertion failure in file /work/10.2/storage/innobase/btr/btr0cur.cc line 1429
       [rr 325332 295750]InnoDB: Failing assertion: !page_zip || page_zip_validate(page_zip, page, index)
       # The prepended   rr <pid> <event number>   might be valuable
       # for especially fast jumping short before the problem.
       # Hence more messages into the server error log could help.
       ....
   ##### So the reason for the assert is somehow between event 14527 or rather 167600 and 295745?
   (rr) run 167600
   run 167600
   The program being debugged has been started already.
   Start it from the beginning? (y or n) y
   Starting program: /home/mleich/work/10.2/bld_debug/sql/mysqld 167600
   2020-01-20 17:57:32 140576640939840 [Note] /work/10.2/bld_debug/sql/mysqld (mysqld 10.2.31-MariaDB-debug-log) starting as process 325332 ...
   ....
   2020-01-20 17:57:52 140575085319936 [Note] InnoDB: Deleting the meta-data file './test/oltp2.cfg'

   --------------------------------------------------
    ---> Reached target process 325332 at event 167602.
   --------------------------------------------------
   warning: unable to open /proc file '/proc/325332/task/325332/maps'
   [New Thread 325332.325332]
   [New Thread 325332.325364]
   ....
   [New Thread 325332.326638]

   Thread 1 stopped.
   0x0000000070000002 in ?? ()
   (rr) b mysql_execute_command    # Maybe     mysql_parse     is better
   Breakpoint 1 at 0x561b803164d4: file /work/10.2/sql/sql_parse.cc, line 2990.
   (rr) continue
   Continuing.
   2020-01-20 17:57:52 140575085319936 [Note] InnoDB: Resuming purge
   [Switching to Thread 325332.326527]

   Thread 40 hit Breakpoint 1, mysql_execute_command (thd=0x7fd9c8000a88) at /work/10.2/sql/sql_parse.cc:2990
   2990    {
   (rr) when
   Current event: 167661
   (rr) run 260000
   The program being debugged has been started already.
   Start it from the beginning? (y or n) y
   Starting program: /home/mleich/work/10.2/bld_debug/sql/mysqld 26000
   ....
   Thread 1 stopped.
   0x0000561b80d4d97d in my_timer_cycles () at /work/10.2/mysys/my_rdtsc.c:170
   170       __asm__ __volatile__ ("rdtsc\n\t" \
   (rr) b mysql_parse
   Breakpoint 2 at 0x561b80325097: file /work/10.2/sql/sql_parse.cc, line 7661.
   (rr) continue
   Continuing.
   [Switching to Thread 325332.326519]

   Thread 36 hit Breakpoint 2, mysql_parse (thd=0x7fd9d8000a88, rawbuf=0x7fd9d8010550 "UPDATE `oltp3` SET `c` = 'b' WHERE `id` = 41905 /* E_R Thread4 QNO 609 CON_ID 20 */", length=83, parser_state=0x7fda300ae250, is_com_multi=false, is_next_command=false)
       at /work/10.2/sql/sql_parse.cc:7661
   7661      DBUG_ENTER("mysql_parse");
   (rr)
   #### In the current example
   # mysql.err contains
   #     [rr 325332 295750]InnoDB: Failing assertion: !page_zip || page_zip_validate(page_zip, page, index)
   #     ....
   #     | Connection ID (thread ID): 17
   #     | [rr 325332 302293]Status: NOT_KILLED
   #     |
   #     ... non nice backtrace
   # So the SQL statement is not known. But its Connection ID 17.
   # The RQG log (sql tracing was enabled, printing happens when the server response or whatever similar arrives) contains
   # ...
   # [326513] [sqltrace] ERROR 1100: UPDATE IGNORE `oltp2` SET `k` = `id` + 1 WHERE `id` = 21606 /* E_R Thread6 QNO 716 CON_ID 22 */ ;
   # [326513] [sqltrace] ERROR 1054: UPDATE `oltp3` SET `pad` = 'k' WHERE `id` = 24988 /* E_R Thread6 QNO 717 CON_ID 22 */ ;
   # [326514] [sqltrace] ERROR 2013: SELECT `pad` FROM `oltp3` WHERE `id` = 57951 /* E_R Thread7 QNO 638 CON_ID 23 */ ;
   # [326515] [sqltrace] ERROR 2013: FLUSH TABLES /* E_R Thread8 QNO 1416 CON_ID 24 */ ;
   # [326513] [sqltrace] ERROR 2013: FLUSH TABLES /* E_R Thread6 QNO 718 CON_ID 22 */ ;
   # [326510] [sqltrace] ERROR 2013: UPDATE `oltp3` SET `c` = 's' WHERE `id` = 11205 /* E_R Thread3 QNO 613 CON_ID 19 */ ;
   # [326509] [sqltrace] ERROR 2013: INSERT IGNORE INTO `oltp3` ( `id` ) VALUES ( NULL ) /* E_R Thread2 QNO 668 CON_ID 18 */ ;
   # [326512] [sqltrace] ERROR 2013: DELETE FROM `oltp3` WHERE `id` = 64960 /* E_R Thread5 QNO 1902 CON_ID 21 */ ;
   #### Some internal stuff from RQG which repeats stuff from above from tracing.
   # 2020-01-20T17:58:40 [326515] Executor::MySQL::execute: Query: FLUSH TABLES /* E_R Thread8 QNO 1416 CON_ID 24 */  failed: 2013 Lost connection to MySQL server during query
   # ....
   # [326511] [sqltrace] ERROR 2013: SELECT SUM(`id`) FROM `oltp3` WHERE `id` BETWEEN 42519 AND 55611 /* E_R Thread4 QNO 662 CON_ID 20 */ ;
   # [326508] [sqltrace] ERROR 2013: UPDATE IGNORE `oltp3` SET `id` = `k` + 1 WHERE `id` = 8740 /* E_R Thread1 QNO 639 CON_ID 17 */ ; <=== Maybe its this statement?
   # ....
   # 2020-01-20T17:58:43 [326507] ERROR: Reporter 'Deadlock': The connect attempt to dsn dbi:mysql:host=127.0.0.1:port=28480:user=root:database=test:mysql_local_infile=1 failed: Can't connect to MySQL server on '127.0.0.1' (111)
   # 2020-01-20T17:58:43 [326507] ERROR: Reporter 'Deadlock': Will return status 101.
   #

   Good luck





Matthias 2021-03

